import streamlit as st
import os
from dotenv import load_dotenv
from langchain_deepseek import ChatDeepSeek
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

# --- 1. åŠ è½½ .env (Day 2) ---
load_dotenv()
api_key = os.getenv("DEEPSEEK_API_KEY")
if not api_key:
    print("æœªæ‰¾åˆ° DeepSeek API Key!")
    # åœ¨ Streamlit ä¸­ï¼Œæˆ‘ä»¬ç”¨ st.error æ¥æ˜¾ç¤ºé”™è¯¯
    st.error("æœªæ‰¾åˆ° DeepSeek API Key! è¯·æ£€æŸ¥ä½ çš„ .env æ–‡ä»¶ã€‚")
    st.stop() # åœæ­¢æ‰§è¡Œ


# --- 2. (æ ¸å¿ƒ) RAG é“¾æ¡åˆ›å»ºå‡½æ•° (Day 5) ---
#
# ğŸŒŸğŸŒŸğŸŒŸ ä½¿ç”¨â€œé­”æ³•â€ @st.cache_resource ğŸŒŸğŸŒŸğŸŒŸ
# å‘Šè¯‰ Streamlitï¼šåªè¿è¡Œè¿™ä¸ªå‡½æ•°ä¸€æ¬¡ï¼Œç„¶åæŠŠç»“æœâ€œå­˜â€èµ·æ¥ã€‚
@st.cache_resource 
def get_rag_chain(file_path: str): # â¬…ï¸ ğŸŒŸ å‡çº§ï¼šæ·»åŠ  file_path å‚æ•°
    print(f"--- ğŸ§  æ­£åœ¨ä¸º {file_path} åˆ›å»º RAG å¼•æ“... (æ­¤è¿‡ç¨‹åªåº”è¿è¡Œä¸€æ¬¡!) ---")
    # 1-4. åŠ è½½ã€åˆ†å‰²ã€åµŒå…¥ã€å­˜å‚¨
    loader = PyPDFLoader(file_path) # â¬…ï¸ ğŸŒŸ å‡çº§ï¼šä½¿ç”¨ä¼ å…¥çš„å‚æ•°
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    splits = text_splitter.split_documents(docs)

    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    embeddings = HuggingFaceEmbeddings(model_name=model_name)

    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)

    # 5. æ£€ç´¢
    retriever = vectorstore.as_retriever()

    # 6. RAG é“¾
    template = """
    ä½ æ˜¯ä¸€ä¸ªé—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æä¾›çš„èµ„æ–™æ¥å›ç­”é—®é¢˜ã€‚
    å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯·å›ç­” "æˆ‘ä¸çŸ¥é“"ã€‚

    èµ„æ–™:
    {context}

    é—®é¢˜:
    {question}
    """
    prompt = ChatPromptTemplate.from_template(template)
    chat = ChatDeepSeek(model="deepseek-chat")
    output_parser = StrOutputParser()

    setup_and_retrieval = RunnableParallel(
        context=retriever,
        question=RunnablePassthrough()
    )

    rag_chain = setup_and_retrieval | prompt | chat | output_parser

    print("--- RAG å¼•æ“åˆ›å»ºå®Œæ¯• ---")
    return rag_chain

# --- 3. (æ ¸å¿ƒ) Streamlit UI (Day 10 ç»ˆæç‰ˆ) ---

st.title("ğŸ¤– Chat with your *own* PDF")

# 3a. (æ–°) åœ¨ä¾§è¾¹æ åˆ›å»ºâ€œæ–‡ä»¶ä¸Šä¼ â€UI
st.sidebar.title("ğŸ“š PDF çŸ¥è¯†åº“")
uploaded_file = st.sidebar.file_uploader("è¯·åœ¨æ­¤å¤„ä¸Šä¼ ä½ çš„ PDF:", type="pdf")

# 3b. (æ–°) åªæœ‰å½“ç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶åï¼Œæ‰æ˜¾ç¤ºèŠå¤©ç•Œé¢
if uploaded_file is not None:
    
    # 1. (æ–°) å°†ä¸Šä¼ çš„æ–‡ä»¶â€œæš‚å­˜â€åˆ°ç£ç›˜
    # (è¿™æ˜¯æœ€ç¨³å¥çš„åšæ³•ï¼Œèƒ½è®© PyPDFLoader æ­£å¸¸è¯»å–)
    temp_dir = "temp_files"
    os.makedirs(temp_dir, exist_ok=True) # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
    temp_file_path = os.path.join(temp_dir, uploaded_file.name)
    
    with open(temp_file_path, "wb") as f:
        f.write(uploaded_file.getbuffer()) # getbuffer() æ˜¯è·å–â€œå†…å­˜ä¸­æ–‡ä»¶â€å†…å®¹çš„æ–¹æ³•
    
    st.sidebar.success(f"æ–‡ä»¶ '{uploaded_file.name}' å·²æˆåŠŸä¸Šä¼ å¹¶ç´¢å¼•ã€‚")

    # 2. (æ–°) â€œæ™ºèƒ½â€è·å– RAG å¼•æ“
    #    @st.cache_resource çš„â€œé­”æ³•â€åœ¨è¿™é‡Œï¼š
    #    - å¦‚æœ `get_rag_chain(temp_file_path)` ä¹‹å‰è¿è¡Œè¿‡ï¼Œå®ƒä¼šâ€œç¬é—´â€è¿”å›ç¼“å­˜çš„å¼•æ“ã€‚
    #    - å¦‚æœè¿™æ˜¯ä¸€ä¸ªâ€œæ–°â€çš„ `temp_file_path`ï¼Œå®ƒä¼šâ€œæ­£å¸¸è¿è¡Œâ€å‡½æ•°ï¼ˆèŠ±1åˆ†é’Ÿï¼‰ï¼Œ
    #      ç„¶åæŠŠâ€œæ–°å¼•æ“â€ç¼“å­˜èµ·æ¥ã€‚
    try:
        rag_chain = get_rag_chain(temp_file_path)
    except Exception as e:
        st.error(f"åˆ›å»º RAG å¼•æ“å¤±è´¥ï¼š {e}")
        st.stop()

    # 3. (æ–°) ä¸ºâ€œæ¯ä¸ªæ–‡ä»¶â€åˆ›å»ºâ€œä¸“å±â€çš„èŠå¤©è®°å½•
    #    (æˆ‘ä»¬æŠŠæ–‡ä»¶åä½œä¸ºâ€œè®°å¿†èŠ¯ç‰‡â€çš„ Key)
    file_specific_history = f"messages_{uploaded_file.name}"
    if file_specific_history not in st.session_state:
        st.session_state[file_specific_history] = []

    # 4. (æ–°) æ˜¾ç¤ºâ€œä¸“å±â€èŠå¤©è®°å½•
    for message in st.session_state[file_specific_history]:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # 5. (æ–°) èŠå¤©è¾“å…¥æ¡†
    if prompt := st.chat_input(f"è¯·æé—®å…³äº '{uploaded_file.name}' çš„é—®é¢˜..."):
        
        # (å­˜å…¥å¹¶æ˜¾ç¤ºâ€œä¸“å±â€ç”¨æˆ·æ¶ˆæ¯)
        st.session_state[file_specific_history].append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # (è°ƒç”¨ RAG å¼•æ“)
        with st.chat_message("assistant"):
            try:
                with st.spinner("ğŸ§  æ­£åœ¨æ€è€ƒå¹¶æ£€ç´¢ä½ ä¸Šä¼ çš„ PDF..."):
                    response_content = rag_chain.invoke(prompt)
                st.markdown(response_content)
                
            except Exception as e:
                response_content = f"è°ƒç”¨ RAG å¼•æ“æ—¶å‡ºé”™ï¼š {e}"
                st.error(response_content)
        
        # (å­˜å…¥â€œä¸“å±â€æœºå™¨äººå›å¤)
        st.session_state[file_specific_history].append({"role": "assistant", "content": response_content})
        
else:
    # (å¦‚æœè¿˜æ²¡ä¸Šä¼ æ–‡ä»¶)
    st.info("ğŸ‘‹ è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ ä¸Šä¼ ä¸€ä¸ª PDF æ–‡ä»¶ï¼Œå¼€å§‹èŠå¤©å§ï¼")